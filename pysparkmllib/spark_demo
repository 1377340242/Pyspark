# _*_ encoding:UTF-8 _*_
# version : Python3.6.5
# author: Liu Zhan
# date: 2018/7/31 22:22

# 导入必要的库

import sys
from time import time
import pandas as pd
from pyspark import SparkConf, SparkContext
from pyspark.mllib.tree import DecisionTree
from pyspark.mllib.regression import LabeledPoint
import numpy as np
from pyspark.mllib.evaluation import BinaryClassificationMetrics


def SetLogger(sc):
    """
    设置显示日志的输出。
    :param sc:
    :return:
    """
    logger = sc._jvm.org.apache.log4j
    logger.LogManager.getLogger("org").setLevel(logger.Level.ERROR)
    logger.LogManager.getLogger("akka").setLevel(logger.Level.ERROR)
    logger.LogManager.getRootLogger().setLevel(logger.Level.ERROR)


def SetPath(sc):
    """
    根据spark的运行模式，决定文件输入路径。
    :param sc:
    :return:
    """
    global Path
    if sc.master[0:5] == "local":
        Path = "file:/home/liuzhan/pythonsparkexample/PythonProject/"
    else:
        Path = "hdfs://master:9000/user/liuzhan/"  # 需要将文件上传到hdfs

# 处理数据


def get_mapping(rdd, idx):
    return rdd.map(lambda fields: fields[idx]).distinct(
    ).zipWithIndex().collectAsMap()

# 提取输出标签


def extract_label(record):
    label = (record[-1])
    return float(label)

# 提取特征值


def extract_features(field, categoriesMap, featureEnd):
    categoryIdx = categoriesMap[field[3]]
    categoryFeatures = np.zeros(len(categoriesMap))
    categoryFeatures[categoryIdx] = 1
    numericalFeatures = [convert_float(field)
                         for field in field[4: featureEnd]]
    return np.concatenate((categoryFeatures, numericalFeatures))


def convert_float(x):
    return (0 if x == "?" else float(x))

# 准备数据


def PrepareData(sc):
    """
    对数据进行处理，划分数据集
    :param sc:
    :return:
    """
    # ----------------------1.导入并转换数据-------------
    print("开始导入数据...")
    rawDataWithHeader = sc.textFile(Path + "data/train.tsv")
    header = rawDataWithHeader.first()
    rawData = rawDataWithHeader.filter(lambda x: x != header)
    rData = rawData.map(lambda x: x.replace("\"", ""))
    lines = rData.map(lambda x: x.split("\t"))
    print("共计：" + str(lines.count()) + "项数据")
    # ----------------------2.建立训练评估所需数据 RDD[LabeledPoint]-------------
    categoriesMap = lines.map(lambda fields: fields[3]). \
        distinct().zipWithIndex().collectAsMap()
    labelpointRDD = lines.map(lambda r: LabeledPoint(
        extract_label(r),
        extract_features(r, categoriesMap, len(r) - 1)))

    # ----------------------3.以随机方式将数据分为3个部分并且返回-------------
    (trainData, validationData,
     testData) = labelpointRDD.randomSplit([8, 1, 1])
    print("将数据分trainData:" + str(trainData.count()) +
          "   validationData:" + str(validationData.count()) +
          "   testData:" + str(testData.count()))
    return (trainData, validationData, testData, categoriesMap)  # 返回数据


def PredictData(sc, model, categoriesMap):
    print("开始导入数据...")
    rawDataWithHeader = sc.textFile(Path + "data/test.tsv")
    header = rawDataWithHeader.first()
    rawData = rawDataWithHeader.filter(lambda x: x != header)
    rData = rawData.map(lambda x: x.replace("\"", ""))
    lines = rData.map(lambda x: x.split("\t"))
    print("共计：" + str(lines.count()) + "项")
    dataRDD = lines.map(lambda r: (r[0],
                                   extract_features(r, categoriesMap, len(r))))
    DescDict = {
        0: "暂时性网页(ephemeral)",
        1: "长青网页(evergreen)"
    }
    for data in dataRDD.take(10):
        predictResult = model.predict(data[1])
        print
        " 网址：  " + str(data[0]) + "\n" + \
            "             ==>预测:" + str(predictResult) + \
            " 说明:" + DescDict[predictResult] + "\n"


def evaluateModel(model, validationData):
    score = model.predict(validationData.map(lambda p: p.features))
    scoreAndLabels = score.zip(validationData.map(lambda p: p.label))
    metrics = BinaryClassificationMetrics(scoreAndLabels)
    AUC = metrics.areaUnderROC
    return (AUC)


def trainEvaluateModel(trainData, validationData,
                       impurityParm, maxDepthParm, maxBinsParm):
    startTime = time()
    # 创建模型
    model = DecisionTree.trainClassifier(
        trainData,
        numClasses=2,
        categoricalFeaturesInfo={},
        impurity=impurityParm,
        maxDepth=maxDepthParm,
        maxBins=maxBinsParm)

    AUC = evaluateModel(model, validationData)
    duration = time() - startTime
# 进行参数的选择


def evalParameter(
        trainData,
        validationData,
        evalparm,
        impurityList,
        maxDepthList,
        maxBinsList):
    """
    手动使用网格搜索的方式进行参数调优。
    :param trainData: 训练数据
    :param validationData: 验证数据
    :param evalparm: 验证的参数
    :param impurityList: 计算信息熵的方法列表
    :param maxDepthList: 最大深度参数列表
    :param maxBinsList: 决策树节点最大分支参数
    :return:
    """
    metrics = [trainEvaluateModel(trainData, validationData,
                                  impurity, maxDepth, maxBins)
               for impurity in impurityList
               for maxDepth in maxDepthList
               for maxBins in maxBinsList]

    if evalparm == "impurity":
        IndexList = impurityList[:]
    elif evalparm == "maxDepth":
        IndexList = maxDepthList[:]
    elif evalparm == "maxBins":
        IndexList = maxBinsList[:]

    df = pd.DataFrame(
        metrics,
        index=IndexList,
        columns=[
            'AUC',
            'duration',
            'impurity',
            'maxDepth',
            'maxBins',
            'model'])
    showchart(df, evalparm, 'AUC', 'duration', 0.5, 0.7)


def showchart(df, evalparm, barData, lineData, yMin, yMax):

    ax = df[barData].plot(
        kind='bar', title=evalparm, figsize=(
            10, 6), legend=True, fontsize=12)
    ax.set_xlabel(evalparm, fontsize=12)
    ax.set_ylim([yMin, yMax])
    ax.set_ylabel(barData, fontsize=12)
    ax2 = ax.twinx()
    ax2.plot(df[[lineData]].values, linestyle='-',
             marker='o', linewidth=2.0, color='r')
    plt.show()


def evalAllParameter(trainData, validationData,
                     impurityList, maxDepthList, maxBinsList):
    metrics = [trainEvaluateModel(trainData, validationData,
                                  impurity, maxDepth, maxBins)
               for impurity in impurityList
               for maxDepth in maxDepthList
               for maxBins in maxBinsList]

    Smetrics = sorted(metrics, key=lambda k: k[0], reverse=True)
    bestParameter = Smetrics[0]

    print("调校后最佳参数：impurity:" + str(bestParameter[2]) +
          "  ,maxDepth:" + str(bestParameter[3]) +
          "  ,maxBins:" + str(bestParameter[4]) +
          "  ,结果AUC = " + str(bestParameter[0]))

    return bestParameter[5]


def parametersEval(trainData, validationData):

    print("----- 评估impurity参数使用 ---------")
    evalParameter(trainData, validationData, "impurity",
                  impurityList=["gini", "entropy"],
                  maxDepthList=[10],
                  maxBinsList=[10])

    print("----- 评估maxDepth参数使用 ---------")
    evalParameter(trainData, validationData, "maxDepth",
                  impurityList=["gini"],
                  maxDepthList=[3, 5, 10, 15, 20, 25],
                  maxBinsList=[10])
    print("----- 评估maxBins参数使用 ---------")
    evalParameter(trainData, validationData, "maxBins",
                  impurityList=["gini"],
                  maxDepthList=[10],
                  maxBinsList=[3, 5, 10, 50, 100, 200])


def CreateSparkContext():

    sparkConf = SparkConf() \
        .setAppName("RunDecisionTreeBinary") \
        .set("spark.ui.showConsoleProgress", "false")
    sc = SparkContext(conf=sparkConf)
    print("master=" + sc.master)
    SetLogger(sc)
    SetPath(sc)
    return (sc)


if __name__ == "__main__":
    print("RunDecisionTreeBinary")
    sc = CreateSparkContext()
    print("==========数据准备阶段===============")
    (trainData, validationData, testData, categoriesMap) = PrepareData(sc)
    trainData.persist()
    validationData.persist()
    testData.persist()
    print("==========训练评估阶段===============")
    (AUC, duration, impurityParm, maxDepthParm, maxBinsParm, model) = \
        trainEvaluateModel(trainData, validationData, "entropy", 10, 200)

    if (len(sys.argv) == 2) and (sys.argv[1] == "-e"):
        parametersEval(trainData, validationData)
    elif (len(sys.argv) == 2) and (sys.argv[1] == "-a"):
        print("-----所有参数训练评估找出最好的参数组合---------")
        model = evalAllParameter(trainData, validationData,
                                 ["gini", "entropy"],
                                 [3, 5, 10, 15, 20, 25],
                                 [3, 5, 10, 50, 100, 200])
    print("==========测试阶段===============")
    auc = evaluateModel(model, testData)
    print("使用test Data测试最佳模型,结果 AUC:" + str(auc))
    print("==========预测数据===============")
    PredictData(sc, model, categoriesMap)
    print(model.toDebugString())
